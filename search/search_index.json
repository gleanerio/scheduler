{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scheduler, AKA Dagster","text":""},{"location":"#about","title":"About","text":"<p>The following is a description of the steps and requirements for building and deploying the docker based workflow implemented in  dagster.</p>"},{"location":"#overview","title":"Overview","text":"<p>The image following provides a broad overview of the elements that  are loaded in to the Docker orchestration environment.  This is a very  basic view and doesn't present any scaling or fail over elements.  </p> <p>The key elements are:</p> <ul> <li>sources to configuration  to load into the Gleaner and Nabu tools, and push to the triplestore. These are now stored in an s3 location</li> <li>gleaner configuration. a list of sources to load. (NOTE: This is also a docker config that needs to be updated to mactch to make things work)</li> <li>tenant configuration. a list communities, and which sources they load</li> <li>The Dagster set which loads three containers to support workflow operations</li> <li>The Gleaner Architecture images which loads three or more containers to support </li> <li>s3 object storage</li> <li>graph database (triplestore)</li> <li>headless chrome for page rendering to support dynamically inserted JSON-LD</li> <li>any other support packages like text, semantic or spatial indexes</li> </ul>"},{"location":"#workflows","title":"WORKFLOWS","text":"<p>There are three workflows * ingest works to load sources * tasks weekly task * ecrr - loads Earthcube Resource Registry</p> <pre><code>---\ntitle: Dagster Stack\n---\nflowchart LR\n    subgraph DockerCompose[Docker Compose Stacks]\n            maincompose[dagster/implents/deployment/compose_project.yaml]\n            project_overrides[dagster/implnets/deployment/compose_project_eco_override.yaml]\n    end\n\n    subgraph Config\n         subgraph s3\n            gleanconfig[gleanerconfig.yaml]\n            tenant[tenant.yaml]\n\n        end\n\n        subgraph Dagster/Config\n            workflow[ eco-wf ]\n            container-config-gleaner[gleanerio contaianer config]\n            container-config-nabu[gleanerio container config for nabu]\n        end\n        env['environment variables']\n\n    end\n    subgraph docker[docker managed by portainer]\n\n        subgraph Containers\n                dagit\n                dagster\n                postgres\n                ingest\n                tasks\n                ecrr\n        end\n        config\n        subgraph Volumes\n            dagster-postgres\n        end\n    end\n    postgres--uses--&gt;dagster-postgres\n    dagster--uses--&gt;workflow\n    dagit--uses--&gt;workflow\n    workflow--&gt;config\n    maincompose--deploys--&gt;dagit[dagster webserver]\n    maincompose--deploys--&gt;dagster[dagster main]\n    maincompose--deploys--&gt;ingest[gleanerio ingest code]\n    maincompose--deploys--&gt;tasks[gleanerio task code]\n    project_overrides--deploys--&gt;ecrr[earthcube code]\n    ingest--reads--&gt;gleanconfig\n    ingest--reads--&gt;tenant\n    tasks--reads--&gt;gleanconfig\n    tasks--reads--&gt;gleanconfig\n    dagster--uses--&gt;postgres\n</code></pre>"},{"location":"#basic-deployment","title":"basic deployment","text":"<ol> <li>information for environment variables is created</li> <li>The configuration files are created and loaded to s3, and docker/config</li> <li>a docker stack is created, and the environment variables are added.</li> <li>portainer deploys containers</li> <li>when ingest and tasks are executed, they read</li> </ol>"},{"location":"#ingest-workflow","title":"Ingest Workflow","text":"<pre><code>---\ntitle: Ingest Workflow Sequence\n---\nsequenceDiagram\n    participant S3\n    participant Ingest\n    participant Portainer\n    participant Graph\n    S3-&gt;&gt;Ingest: read sources from scheduler/configs/gleanerconfig.yaml\n    S3-&gt;&gt;Ingest: read tenant from scheduler/configs/tenant.yaml\n    Ingest--&gt;Ingest: create gleanerio container\n    Ingest-&gt;&gt;Portainer: run gleanerio \n    Portainer--&gt;Portainer: docker configs mounted in gleanerio container\n    Portainer--&gt;Portainer: summon for sources\n    Portainer-&gt;&gt;S3: jsonld  to s3\n    Portainer-&gt;&gt;Ingest:  logs returned\n    Ingest-&gt;&gt;S3: logs from run to S3\n    Ingest-&gt;&gt;Ingest: create load reports using EC Utils\n    Ingest-&gt;&gt;S3: load reports  to s3\n    Ingest-&gt;&gt;Portainer: run nabu  to \n    Portainer--&gt;Portainer: convert jsonld  to release and release summary\n    Portainer-&gt;&gt;S3: release and release summary  to s3\n    Ingest-&gt;&gt;Ingest: create graph report using EC Utils\n    Ingest-&gt;&gt;S3: graph report  to s3\n    Ingest-&gt;&gt;Graph: Create a namespaces for tenant\n    Ingest-&gt;&gt;Graph: load release and release summary to namespaces</code></pre> <pre><code>---\ntitle: Ingest Simplified Flowchart \n---\nflowchart LR\n    subgraph config\n          s3_config_sensors\n    end    \n    subgraph jobs\n        summon_and_release\n        tenant_release\n    end\n    subgraph assets\n        sources\n        tenants\n    end\n\n\n\n    s3_config_sensors--monitors --&gt; configs\n    s3_config_sensors--writes  --&gt;sources \n    s3_config_sensors--writes  --&gt;tenants\n    summon_and_release--uses--&gt;sources --runs --&gt; gleanerio\n    tenant_release--uses--&gt;tenants --runs --&gt; tenant_release\n    gleanerio--stores JSONLD --&gt;summon\n    gleanerio--stores log --&gt;logs\n    summon_and_release-- reads --&gt; summon\n    summon_and_release-- converts to graph  --&gt;graph_path\n    tenant_release -- monitors --&gt; graph_path\n    tenant_release -- loads releases to --&gt; tenant_namespace\n    tenant_release -- loads releases to --&gt; tenant_summary_namespace\n\n\n    subgraph portainer\n      gleanerio \n      tenant_ui\n      end\n      subgraph services\n           triplestore\n               tenant_namespace\n               tenant_summary_namespace\n            end\n\n          subgraph minio_s3 \n            subgraph bucket_paths\n                subgraph scheduler\n                     configs[\"`scheduler/configs`\"]\n                     logs\n                end\n                summon\n                graph_path['graph']\n            end\n            end\n\n\n\n\n</code></pre>"},{"location":"#task-workflows","title":"Task workflows","text":"<pre><code>---\ntitle: Task Workflow Sequence\n---\nsequenceDiagram\n    participant S3\n    participant Ingest\n    participant Portainer\n    participant Graph\n    Ingest-&gt;&gt;Ingest: all_graph_stats assets: graph statistics using EC Utils\n    Ingest-&gt;&gt;S3: load all_graph_stats  to s3\n    Ingest-&gt;&gt;Ingest: source_stats assets: loadstatsHistory using EC Utils\n    Ingest-&gt;&gt;Graph: sparql query to get graph stats\n    Graph-&gt;&gt;Ingest: results for source_stats\n    Ingest-&gt;&gt;S3: source_stats  to s3\n</code></pre>"},{"location":"#steps-to-build-and-deploy","title":"Steps to build and deploy","text":"<p>The deployment can be tested locally. You can setup a services stack in docker to locally test, or use existing  services.</p> <p>The production 'containers' dagster, gleaner, and nabu are built with a github action. You can also use  a makefile.</p> <p>This describes the local and container deployment We use portainer to manage our docker deployments.</p>"},{"location":"#server-deployment","title":"Server Deployment.","text":"<p>Production example for Earthcube </p>"},{"location":"#developer-pycharm-run-local-with-remote-services","title":"DEVELOPER Pycharm --  Run local with remote services","text":"<p>You can test components in pycharm. Run configurations for pycgharm  are in runConfigurations (TODO: Instructions) use the ENVFIle plugin.  1) move to the  implnets/deployment directory 2) copy the envFile.env to .env see  use the ENVFIle plugin. 3) edit the entries to point at a portainer/traefik with running services 4) edit configuration files in implnets/configs/PROJECT: gleanerconfig.yaml, tenant.yaml 5) upload configuration implnets/configs/PROJECT to s3 scheduler/configs: gleanerconfig.yaml, tenant.yaml 4) run a Pycharm runconfig     5) eg dagster_ingest_debug 4) go to http://localhost:3000/ 6) you can test the schedules </p>"},{"location":"#full-stack-test-run-local-with-remote-services","title":"full stack test Run local with remote services","text":"<p>1) move to the implnets/deployment directory 2) copy the envFile.env to .env seeuse the ENVFIle plugin. see  use the ENVFIle plugin.  3) edit the entries. 4) edit configuration files in implnets/configs/PROJECT to s3: gleanerconfig.yaml, tenant.yaml 5) upload configuration implnets/configs/PROJECT to scheduler/configs s3: gleanerconfig.yaml, tenant.yaml 4) for local, <code>./dagster_localrun.sh</code> 5) go to http://localhost:3000/</p> <p>To deploy in portainer, use the deployment/compose_project.yaml docker stack.</p>"},{"location":"#docker-compose-configuration","title":"docker compose Configuration:","text":"<p>there are configuration  files that are needed. They are installed in two places: * as docker configs * as scheduler configs in S3</p> <p>(NOTE: I think the configs are still needed in the containers) </p> file local note workspace configs/PROJECT/worksapce.yaml dockerconfig: workspace docker compose: used by dagster gleanerconfig.yaml configs/PROJECT/gleanerconfig.yaml s3:{bucket}/scheduler/configs/gleanerconfigs.yaml ingest workflow needs to be in minio/s3 tenant.yaml configs/PROJECT/tenant.yaml s3:{bucket}/scheduler/configs/tenant.yaml ingest workflow needs to be in minio/s3 dagster.yaml dagster/implnets/deployment/dagster.yaml dockerconfig: dagster docker compose: used by dagster gleanerconfig.yaml configs/PROJECT/gleanerconfig.yaml dockerconfig: gleaner mounted in gleaner docker container nabuconfig.yaml configs/PROJECT/nabuconfig.yaml dockerconfig: nabu mounted in gleaner docker container <p>(NOTE: This is also a gleaner config (below in runtime configuration) that needs to be updated to mactch to make things work)</p> <p>Docker Configs for gleanerio containers  are still needed:</p> file local stack note gleanerconfig.yaml configs/PROJECT/gleanerconfigs.yaml env () generated code needs to be in ~~portainer~~ nabuconfig.yaml configs/PROJECT/nabuconfigs.yaml env () generated codeneeds to be in ~~portainer~~ <p>3) when the containers are running in a  stack, on portainer, there will need to    be updated by pulling from dockerhub. The ENV variables may need to be updated for the CONTAINER*_TAG</p>"},{"location":"#runtime-configuration","title":"Runtime configuration","text":""},{"location":"#upload-to-an-s3-bucket","title":"upload to an s3 bucket","text":"file local note gleanerconfig.yaml s3:{bucket}/scheduler/configs/gleanerconfigs.yaml ingest workflow needs to be in minio/s3 tenant.yaml s3:{bucket}/scheduler/configs/enant.yaml ingest workflow needs to be in minio/s3"},{"location":"#updating-config","title":"updating config","text":"<p>You can update a config, and a sensor should pick up the changes. 1) Upload changed file to s3    2) note, if this is a new source, you need to add it to the docker config (gleaner-PROJECT).  2) go to overview,  3) go to  s3_config_source_sensor  for gleanerconfig.yaml changes, and s3_config_tenant_sensor for tenant.yaml changes  . 4) at some point, a run should occur.  . 5) then go to the sources_sensor, or tenant sensor  if job does not run, you can do a backfill.</p>"},{"location":"#new-sources","title":"new sources:","text":"<p>6)  so to job tab, and run summon_and_release with the 'partitions' aka 'sources' that are recent. 7) click materialize_all, and in the backfill dialog be sure only the added partition is selected.  . 8) go to runs, and see that a job with a partition with that name is queued/running 9) run tenant_release_job with same partition name to load data to tenants</p>"},{"location":"#_1","title":"Scheduler","text":""},{"location":"#new-tenants","title":"new tenants:","text":"<p>There are two jobs that need to run to move data to a tenant. (third will be needed for UI) 6)  so to job tab, and run tenant_namespaces_job with the 'partitions' aka 'tenant' that are recent.' 7) click materialize_all, and be sure only the added partition is selected 8) go to runs, and see that a job with a partition with that name is queded,/running 6)  so to job tab, and run tenant_release_job with the 'partitions' aka 'sources' for that tenant 7) click materialize_all, The data will be pushed to all tenant namespaces</p>"},{"location":"#test-schedules","title":"test schedules","text":""},{"location":"#environment-files","title":"Environment files","text":"<p>1) cp deployment/envFile.env .env 2) edit 3) <code>export $(cat .env | xargs)</code> export $(cat .env | xargs) <pre><code>######\n# Nabu and Gleaner configs need to be in docker configs\n## docker config name GLEANER_GLEANER_DOCKER_CONFIG\n## docker config name GLEANER_NABU_DOCKER_CONFIG\n#        suggested DOCKER_CONFIG NAMING PATTERN (nabu||gleaner)-{PROJECT}\n########\nGLEANERIO_DOCKER_GLEANER_CONFIG=gleaner-eco\nGLEANERIO_DOCKER_NABU_CONFIG=nabu-eco\n\n# ###\n# workspace for dagster\n####\nGLEANERIO_WORKSPACE_CONFIG_PATH=/usr/src/app/workspace.yaml\nGLEANERIO_DOCKER_WORKSPACE_CONFIG=workspace-eco\n\n\n\nDEBUG_CONTAINER=false\n\n#### HOST\n#  host base name for treafik. fixed to localhost:3000 when using  compose_local.\nHOST=localhost\n# Applies only to compose_project.yaml runs\n\n#  modify SCHED_HOSTNAME is you want to run more than one instance\n#    aka two different project havests for now.\nSCHED_HOSTNAME=sched\n\nGLEANERIO_DOCKER_CONTAINER_WAIT_TIMEOUT=300\n# debugging set to 10 - 30 seconds\n\n\nPROJECT=eco\n#PROJECT=iow\n#PROJECT=oih\n# tags for docker compose\nCONTAINER_CODE_TAG=latest\nCONTAINER_DAGSTER_TAG=latest\n\nPROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n# port is required: https://portainer.{HOST}:443/api/endpoints/2/docker/\nGLEANERIO_DOCKER_URL=https://portainer.{HOST}:443/api/endpoints/2/docker/\nGLEANERIO_PORTAINER_APIKEY=\n# if running dagster-dev, then this needs to be set ,\n#       defaults to \"/scheduler/gleanerconfig.yaml\" which is path to config mounted in containers\n# when debugging generated code \"../../../configs/eco/gleanerconfig.yaml\"\n# when debugging code in workflows \"../../configs/eco/gleanerconfig.yaml\"\nGLEANERIO_DAGSTER_CONFIG_PATH=../../../configs/eco/gleanerconfig.yaml\n\n# Network\nGLEANERIO_DOCKER_HEADLESS_NETWORK=headless_gleanerio\n\n### GLEANER/NABU Dockers\nGLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:dev_ec\nGLEANERIO_NABU_IMAGE=nsfearthcube/nabu:dev_eco\n\n##\n# path where configs are deployed/mounted\n####\nGLEANERIO_GLEANER_CONFIG_PATH=/gleaner/gleanerconfig.yaml\nGLEANERIO_NABU_CONFIG_PATH=/nabu/nabuconfig.yaml\n###\n#path in s3 for docker log files\nGLEANERIO_LOG_PREFIX=scheduler/logs/\n\nGLEANERIO_MINIO_ADDRESS=\nGLEANERIO_MINIO_PORT=80\nGLEANERIO_MINIO_USE_SSL=false\nGLEANERIO_MINIO_BUCKET=\nGLEANERIO_MINIO_ACCESS_KEY=\nGLEANERIO_MINIO_SECRET_KEY=\nGLEANERIO_HEADLESS_ENDPOINT=http://headless:9222\n\n# just the base address, no namespace https://graph.geocodes-aws-dev.earthcube.org/blazegraph\nGLEANERIO_GRAPH_URL=https://graph.geocodes-aws.earthcube.org/blazegraph\nGLEANERIO_GRAPH_NAMESPACE=mytest\n\n# optional: GLEANERIO_GRAPH_SUMMARY_ENDPOINT defaults to GLEANERIO_GRAPH_URL\n#GLEANERIO_GRAPH_SUMMARY_ENDPOINT=https://graph.geocodes-aws-dev.earthcube.org/blazegraph\nGLEANERIO_GRAPH_SUMMARY_NAMESPACE=mytest_summary\nGLEANERIO_GRAPH_SUMMARIZE=True\n\n# where are the gleaner and tennant configurations\nGLEANERIO_CONFIG_PATH=\"scheduler/configs/\"\nGLEANERIO_TENANT_FILENAME=\"tenant.yaml\"\nGLEANERIO_SOURCES_FILENAME=\"gleanerconfig.yaml\"\n\n# ECO Custom variables for ecrr\nECRR_GRAPH_NAMESPACE=ecrr\nECRR_MINIO_BUCKET=ecrr\n\n# only a public slack channel works. DV has no permissions to create a new channel\n#SLACK_CHANNEL=\"#production_discussion\"\nSLACK_CHANNEL=\"#twitterfeed\"\nSLACK_TOKEN=\n</code></pre></p>"},{"location":"#appendix","title":"Appendix","text":""},{"location":"#portainer-api-setup","title":"Portainer API setup","text":"<p>You will need to setup Portainer to allow for an API call.  To do this look  at the documentation for Accessing the Portainer API</p>"},{"location":"#notes","title":"Notes","text":""},{"location":"#handle-multiple-organizations","title":"Handle Multiple Organizations","text":"<p>thoughts... </p> <ul> <li>Each organization can be in a container with its own code workflow. </li> <li>in the workflows directory: <code>dagster project projectname</code></li> <li>If we can standardize the loading and transforming workflows as much as possible, then the graph loading workflows   should be standardized. We could just define an additional container in a compose file, and add that to the workflows</li> </ul> <pre><code>load_from:\n#      - python_file:\n#          relative_path: \"project/eco/repositories/repository.py\"\n#          location_name: project\n#          working_directory: \"./project/eco/\"\n#      - python_file:\n#          relative_path: \"workflows/ecrr/repositories/repository.py\"\n#          working_directory: \"./workflows/ecrr/\"\n      # module starting out with the definitions api\n     # - python_module: \"workflows.tasks.tasks\"\n\n      - grpc_server:\n            host: dagster-code-tasks\n            port: 4000\n            location_name: \"tasks\"\n      - grpc_server:\n            host: dagster-code-eco-ingest\n            port: 4000\n            location_name: \"ingest\"\n      - grpc_server:\n            host: dagster-code-ios-ingest\n            port: 4000\n            location_name: \"ingest\"\n      - grpc_server:\n            host: dagster-code-eco-ecrr\n            port: 4000\n            location_name: \"ecrr\"</code></pre> <ul> <li>to add a container, you need to edit the workflows.yaml in an organizations configuration</li> </ul>"},{"location":"#cron-notes","title":"Cron Notes","text":"<p>A useful on-line tool:  https://crontab.cronhub.io/</p> <pre><code>0 3 * * *   is at 3 AM each day\n\n0 3,5 * * * at 3 and 5 am each day\n\n0 3 * * 0  at 3 am on Sunday\n\n0 3 5 * *  At 03:00 AM, on day 5 of the month\n\n0 3 5,19 * * At 03:00 AM, on day 5 and 19 of the month\n\n0 3 1/4 * * At 03:00 AM, every 4 days</code></pre>"},{"location":"#indexing-approaches","title":"Indexing Approaches","text":"<p>The following approaches</p> <ul> <li>Divide up the sources by sitemap and sitegraph</li> <li>Also divide by production and queue sources</li> </ul> <p>The above will result in at most 4 initial sets.</p> <p>We can then use the docker approach</p> <pre><code>./gleanerDocker.sh -cfg /gleaner/wd/rundir/oih_queue.yaml  --source cioosatlantic</code></pre> <p>to run indexes on specific sources in these configuration files.  </p>"},{"location":"README_LOCAL_DEVELOPMENT/","title":"Development","text":"<p>If you look in the doc/README.md that description is probably better.</p> <p>Two types:</p> <p>2) dagster dev   - Dagster runs the UI in development mode 1) Container based. This uses docker and locally deployed containers</p> <p>!!!  note      NOTE, the Dagster and the Code containers need to be the same.     For local development images are named <code>dagster-local:latest</code> and code containers named dagster-gleanerio-local:latest     and built in the compose_local.yaml     for production,       * dagster named: nsfearthcube/dagster-gleanerio:${CONTAINER_DAGSTER_TAG:-latest}       * code containers  are named <code>nsfearthcube/dagster-gleanerio-${PROJECT:-eco}:${CONTAINER_TAG:-latest}</code>     eg in dockerhub.com as nsfearthcube/dagster-eco:latest</p>"},{"location":"README_LOCAL_DEVELOPMENT/#dagster-dev","title":"DAGSTER DEV","text":"<p>At the top level (dagster/implents) you can run </p> <p><code>dagster dev</code></p> <p>You need to set the environment based on dagster/implnets/deployment/envFile.env</p> <p>It should run workflows/tasks/tasks</p> <p>defined in the pyproject.toml</p> <pre><code>[tool.dagster]\nmodule_name = \"workflows.tasks.tasks\"</code></pre>"},{"location":"README_LOCAL_DEVELOPMENT/#setting-up-in-pycharm","title":"Setting up in pycharm","text":"<p>you can add runconfigs in pycharm </p> <p>You should/need to add the envFile plug in so that env files can be  </p>"},{"location":"README_LOCAL_DEVELOPMENT/#testing-tasks","title":"testing tasks","text":"<p><code>cd dagster/implnets/workflows/tasks</code> You need to set the environment based on dagster/implnets/deployment/envFile.env</p> <p><code>export $(sed  '/^[ \\t]*#/d' ../../deployment/.env |  sed '/^$/d' | xargs)</code></p> <p><code>dagster dev</code></p> <p>will run just the task, and in editable form, i think.</p>"},{"location":"README_LOCAL_DEVELOPMENT/#testing-containers","title":"TESTING CONTAINERS","text":"<p>Containers are a well tested approach. We deploy these container to production, so it's a good way to test. There are a set of required files:</p> <ul> <li>env variables file</li> <li>gleaner/nabu configuration files, without any passwords, servers. Those are handled in the env variables</li> <li>docker compose file</li> <li>docker networks and volumes for the compose files</li> <li>three files uploaded to docker as configs<ul> <li>gleanerconfigs.yaml gleaner/nabu</li> <li>nabuconfigs.yaml - gleaner/nabu</li> <li>workspace.yaml -- dagster</li> </ul> </li> <li>(opptional/advanced) add a compose_project_PROJECT_override.yaml file with additional containers</li> </ul>"},{"location":"README_LOCAL_DEVELOPMENT/#portainer-api-key","title":"PORTAINER API KEY","text":"<p>note on how to do this. ''</p>"},{"location":"README_LOCAL_DEVELOPMENT/#start","title":"Start","text":"<p>For production environments, script, <code>dagster_setup_docker.sh</code>  should create the networks, volumes, and  upload configuration files</p> <p>1) setup a project in configs directory, if one des not exist     2)   add gleanerconfig.yaml, nabuconfig.yaml, and workspace.yaml (NOTE NEED A TEMPLATE FOR THIS) 1) copy envFile.env to .env, and edit 2) run  ./dagster_localrun.sh 4) go to https://loclahost:3000/ 5) run a small test dataset.</p> <pre><code>cd dagster/implnets/deployment\ncp envFile.env .env\n# configure environment in .env \n\n./dagster_localrun.sh\n</code></pre> <p>If you look in dagster_localrun.sh you can see that the  $PROJECT variable is used to define what files to use, and define, and to setup a separate 'namespace' in traefik labels.</p> <p>If you look in compose_local_eco_override.yaml you can see that additional mounts are added to the containers.</p> <p>These can be customized in the  <code>compose_local_PROJECT_override.yaml</code> for local development.</p>"},{"location":"README_LOCAL_DEVELOPMENT/#customizing-the-configs","title":"customizing the configs","text":"<p>for local development three configs</p> <ul> <li>configs/PROJECT/gleanerconfigs.yaml gleaner/nabu</li> <li>configs/PROJECT/nabuconfigs.yaml - gleaner/nabu</li> <li>configs/PROJECT/workspace.yaml -- dagster</li> </ul>"},{"location":"README_LOCAL_DEVELOPMENT/#editingtesting-code","title":"Editing/testing code","text":"<p>if you run pygen, then you need to regnerate code. the makefile or a pycharm run config is the best way. </p>"},{"location":"README_LOCAL_DEVELOPMENT/#moving-to-production","title":"MOVING TO PRODUCTION","text":"<p>(NOTE NEED SOME MAKEFILES FOR THIS.)</p> <p>you need to create a compose_project_PROJECT_override.yaml</p> <p>After copying fragment from <code>compose_local_PROJECT_override.yaml</code> 1) CHANGE THE IMAGE TO <code>docker.io/nsfearthcube/dagster-gleanerio-${PROJECT:-eco}:${CONTAINER_CODE_TAG:-latest}</code> 2) remove the line: platform: linux/x86_64</p>"},{"location":"README_LOCAL_DEVELOPMENT/#for-portainer","title":"For portainer,","text":"<p>Create a stack, and add override file using  \"additional_file\" to add this to the stack</p>"},{"location":"README_LOCAL_DEVELOPMENT/#command-line-deploy","title":"command line deploy","text":"<p>docker compose -env .env -f compose_project.yaml -f compose_project_PROJECT_override.yaml up</p>"},{"location":"README_LOCAL_DEVELOPMENT/#system-not-supporting-multiple-configs","title":"system not supporting multiple configs","text":"<p>If you are not using portainer, you need to create a merged config file.</p> <p>Then you will merge the files. Preview with: </p> <p><code>docker compose -f compose_project.yaml -f compose_project_PROJECT_override.yaml config</code></p> <p>this should show you  a merged file.</p> <p><code>docker compose -f compose_project.yaml -f compose_project_PROJECT_override.yaml config  &gt; compose_project_PROJECT.yaml</code></p> <p>then start docker compose with the merged file.</p>"},{"location":"alternatives/","title":"Alternative Approaches","text":""},{"location":"alternatives/#abouts","title":"Abouts","text":"<p>These are just some scratch notes on some of the approaches for  job workflows.  This is a more historical document than anything. </p>"},{"location":"alternatives/#options","title":"Options","text":"<ul> <li>Dagster<ul> <li>Dagster</li> <li>Build ETL pipelines</li> <li>Create new project</li> </ul> </li> <li>Red Engine<ul> <li>Red Engine</li> <li>HN Posting on Red Engine</li> <li>Red Engine read the docs</li> <li>Rocketry</li> </ul> </li> <li>Dramtiq<ul> <li>Dramatic</li> </ul> </li> <li>Temporalio<ul> <li>Temporalio</li> </ul> </li> <li>Luigi<ul> <li>Luigi</li> </ul> </li> <li>AppScheduler<ul> <li>AppScheduler</li> </ul> </li> <li>Go Cadance</li> <li>Airflow</li> <li>Background jobs in linux</li> <li>exec-command</li> </ul>"},{"location":"alternatives/#other-dagster-references","title":"Other Dagster References","text":"<ul> <li>pybokeh/dagster-sklearn</li> <li>Gave me the inspiration for the primary folder structure. Although that     example is more advanced and utilizes sklearn.</li> <li>dagster-io/dagster examples</li> <li>Dagster's own examples.</li> <li>xyzy-web/dagster-exchangerates</li> <li>An example that includes Kubernetes Deployment.</li> <li>sephib/dagster-graph-project</li> <li>sspaeti-com/practical-data-engineering</li> </ul>"},{"location":"developement_uisng_generated_code/","title":"Scheduler Developement in Dagster","text":"<p>NOTE: originally, a set of workflows was generated for each source. These were compiled into separate 'project' containers by a github workflo This is no longer needed. But these were the original instructions.</p> <p>Note</p> <p>add envfile plug in to PyCharm to allow for easy debugging to code</p>"},{"location":"developement_uisng_generated_code/#about","title":"About","text":"<p>The following is a description of the steps and requirements for building and deploying the docker based workflow implemented in  dagster.</p> <p>when developing, you can use <code>dagster dev</code> you need to have the environment variables defined, so it's easist to do in a Pycharm run shell script.</p> <p>You can use a python module, dagster  with option dev load the runConfiguration dagster_eco_debug.run.xml</p>"},{"location":"developement_uisng_generated_code/#template-files","title":"Template files","text":"<p>The template files define the Dagster Ops, Jobs and Schedules.  From these and a GleanerIO config file a set of Python scripts for Dagster are created in the output directory. </p> <p>These only need to be changed or used to regenerate if you wish to alter the  execution graph (ie, the ops, jobs and schedules) or change the config file. In the later case only a regeneration needs to be done.</p> <p>There are then Docker build scripts to build out new containers.  </p> <p>See:  template</p>"},{"location":"developement_uisng_generated_code/#steps-to-build-and-deploy","title":"Steps to build and deploy","text":"<p>The deployment can be tested locally using docker. The production 'containers' are built with a github action, or using a makefile.</p> <p>This describes the local and container deployment We use portainer to manage our docker deployments.</p> <p>1) move to the the deployment directory 2) copy the envFile.env to .env  3) edit the entries. 4) for local, <code>./dagster_localrun.sh</code> 5) go to http://localhost:3000/</p> <p>To deploy in portainer, use the deployment/compose_project.yaml docker stack.</p>"},{"location":"developement_uisng_generated_code/#docker-compose-configuration","title":"docker compose Configuration:","text":"<p>1) there are three files that need to be installed into docker configs. </p> file local stack note workspace configs/PROJECT/worksapce.yaml env () used by dagster gleanerconfig.yaml configs/PROJECT/gleanerconfigs.yaml env () needs to be in portainer nabuconfig.yaml configs/PROJECT/nabuconfigs.yaml env () needs to be in portainer 2)"},{"location":"developement_uisng_generated_code/#editing-template","title":"Editing Template","text":"<p>you can edit implnets/template</p> <p>then deploy with</p> <p>`pygen.py -cf ./configs/eco/gleanerconfig.yaml -od ./generatedCode/implnet-eco/output -td ./templates/v1 -d 7 ``</p> <p>If you are running using dagster_localrun.sh  1) go to the deployment at http://localhost:3000/locations 2) click 'reload on gleaner@project_grpc' 3) then if code is correct, then you will be able run the changed workflows</p> <p>(TODO NEEDS MORE )</p>"},{"location":"developement_uisng_generated_code/#environment-files","title":"Environment files","text":"<p>1) cp deployment/envFile.env .env 2) edit 3) <code>export $(cat .env | xargs)</code> export $(cat .env | xargs) <pre><code>######\n# Nabu and Gleaner configs need to be in docker configs\n## docker config name GLEANER_GLEANER_DOCKER_CONFIG\n## docker config name GLEANER_NABU_DOCKER_CONFIG\n#        suggested DOCKER_CONFIG NAMING PATTERN (nabu||gleaner)-{PROJECT}\n########\nGLEANERIO_GLEANER_DOCKER_CONFIG=gleaner-eco\nGLEANERIO_NABU_DOCKER_CONFIG=nabu-eco\n\n# ###\n# workspace for dagster\n####\nGLEANERIO_WORKSPACE_CONFIG_PATH=/usr/src/app/workspace.yaml\nGLEANERIO_WORKSPACE_DOCKER_CONFIG=workspace-eco\n\n\nDEBUG=False\nGLEANERIO_CONTAINER_WAIT_SECONDS=300\n# debuggin set to 5 or 10 seconds\nPROJECT=eco\nCONTAINER_CODE_TAG=latest\nCONTAINER_DAGSTER_TAG=latest\n#PROJECT=iow\n#PROJECT=oih\nHOST=localhost\nPROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n# port is required: https://portainer.{HOST}:443/api/endpoints/2/docker/\nPORTAINER_URL=\nPORTAINER_KEY=\n# if running dagster-dev, then this needs to be set ,\n#       defaults to \"/scheduler/gleanerconfig.yaml\" which is path to config mounted in containers\n# when debugging generated code \"../../../configs/eco/gleanerconfig.yaml\"\n# when debugging code in workflows \"../../configs/eco/gleanerconfig.yaml\"\n# DAGSTER_GLEANER_CONFIG_PATH=../../../configs/eco/gleanerconfig.yaml\nGLEANERIO_CONTAINER_WAIT_SECONDS=3600\n#GLEANERIO_CONTAINER_WAIT_SECONDS=30\n# Network\nGLEANERIO_HEADLESS_NETWORK=headless_gleanerio\n\n### GLEANER/NABU Dockers\nGLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:latest\nGLEANERIO_NABU_IMAGE=nsfearthcube/nabu:latest\n\n##\n# path where configs are deployed/mounted\n####\nGLEANERIO_GLEANER_CONFIG_PATH=/gleaner/gleanerconfig.yaml\nGLEANERIO_NABU_CONFIG_PATH=/nabu/nabuconfig.yaml\n###\n#path in s3 for docker log files\nGLEANERIO_LOG_PREFIX=scheduler/logs/\n\nGLEANERIO_MINIO_ADDRESS=\nGLEANERIO_MINIO_PORT=80\nGLEANERIO_MINIO_USE_SSL=false\nGLEANERIO_MINIO_BUCKET=\nGLEANERIO_MINIO_ACCESS_KEY=\nGLEANERIO_MINIO_SECRET_KEY=\nGLEANERIO_HEADLESS_ENDPOINT=http://headless:9222\n\n# just the base address, no namespace https://graph.geocodes-aws-dev.earthcube.org/blazegraph\nGLEANERIO_GRAPH_URL=\nGLEANERIO_GRAPH_NAMESPACE=\n\n# example: https://graph.geocodes.ncsa.illinois.edu/blazegraph/namespace/yyearthcube2/sparql\n#graph endpoint will be GLEANERIO_GRAPH_URL\nGLEANERIO_SUMMARY_GRAPH_NAMESPACE=\nGLEANERIO_SUMMARIZE_GRAPH=True\n</code></pre></p>"},{"location":"developement_uisng_generated_code/#implementation-networks","title":"Implementation Networks","text":"<p>This (https://github.com/sharmasagar25/dagster-docker-example)  is an example on how to structure a [Dagster] project in order to organize the jobs, repositories, schedules, and ops. The example also contains examples on unit-tests and a docker-compose deployment file that utilizes a Postgresql database for the run, event_log and schedule storage.</p> <p>This example should in no way be considered suitable for production and is merely my own example of a possible file structure. I personally felt that it was difficult to put the Dagster concepts to use since the projects own examples had widely different structure and was difficult to overview as a beginner.</p> <p>The example is based on the official [tutorial].</p>"},{"location":"developement_uisng_generated_code/#folders","title":"Folders","text":"<ul> <li>build:  build directives for the docker containers</li> <li>configs</li> <li>src</li> <li>tooling</li> </ul>"},{"location":"developement_uisng_generated_code/#requirements","title":"Requirements","text":"<p>At this point it is expected that you have a valid Gleaner config file named gleanerconfig.yaml located in some path within the configs directory.</p>"},{"location":"developement_uisng_generated_code/#building-the-dagster-code-from-templates","title":"Building the dagster code from templates","text":"<p>The python program pygen will read a gleaner configuration file and a set of  template and build the Dagster code from there.  </p> <pre><code>python pygen.py -cf ./configs/nsdf/gleanerconfig.yaml -od ./src/implnet-nsdf/output  -td ./src/implnet-nsdf/templates  -d 7</code></pre>"},{"location":"developement_uisng_generated_code/#running","title":"Running","text":"<p>There is an example on how to run a single pipeline in <code>src/main.py</code>. First install the dependencies in an isolated Python environment.</p> <pre><code>pip install -r requirements</code></pre> <p>The code built above can be run locally, though your templates may be set up  to reference services and other resources not present on your dev machine.  For  complex examples like these, it can be problematic.  </p> <p>If you are looking for some simple examples of Dagster, check out the directory examples for some smaller self-contained workflows.  There are good for testing things like sensors and other approaches. </p> <p>If you wish to still try the generated code cd into the output directory you specified in the pygen command.</p> <p>Then use:</p> <pre><code>dagit -h ghost.lan -w workspace.yaml</code></pre>"},{"location":"developement_uisng_generated_code/#building","title":"Building","text":"<pre><code> podman build  -t  docker.io/fils/dagster:0.0.24  .</code></pre> <pre><code> podman push docker.io/fils/dagster:0.0.24</code></pre>"},{"location":"developement_uisng_generated_code/#appendix","title":"Appendix","text":""},{"location":"developement_uisng_generated_code/#setup","title":"Setup","text":""},{"location":"developement_uisng_generated_code/#docker-api-sequence","title":"Docker API sequence","text":""},{"location":"developement_uisng_generated_code/#appendix_1","title":"Appendix","text":""},{"location":"developement_uisng_generated_code/#portainer-api-setup","title":"Portainer API setup","text":"<p>You will need to setup Portainer to allow for an API call.  To do this look  at the documentation for Accessing the Portainer API</p>"},{"location":"developement_uisng_generated_code/#notes","title":"Notes","text":"<p>Single file testing run</p> <pre><code> dagit -h ghost.lan -f test1.py</code></pre> <ul> <li>Don't forget to set the DAGSTER_HOME dir like in </li> </ul> <pre><code> export DAGSTER_HOME=/home/fils/src/Projects/gleaner.io/scheduler/python/dagster</code></pre> <pre><code>dagster-daemon run</code></pre> <p>Run from directory where workspace.yaml is. <pre><code>dagit --host 192.168.202.159</code></pre></p>"},{"location":"developement_uisng_generated_code/#cron-notes","title":"Cron Notes","text":"<p>A useful on-line tool:  https://crontab.cronhub.io/</p> <pre><code>0 3 * * *   is at 3 AM each day\n\n0 3,5 * * * at 3 and 5 am each day\n\n0 3 * * 0  at 3 am on Sunday\n\n0 3 5 * *  At 03:00 AM, on day 5 of the month\n\n0 3 5,19 * * At 03:00 AM, on day 5 and 19 of the month\n\n0 3 1/4 * * At 03:00 AM, every 4 days</code></pre>"},{"location":"developement_uisng_generated_code/#indexing-approaches","title":"Indexing Approaches","text":"<p>The following approaches</p> <ul> <li>Divide up the sources by sitemap and sitegraph</li> <li>Also divide by production and queue sources</li> </ul> <p>The above will result in at most 4 initial sets.</p> <p>We can then use the docker approach</p> <pre><code>./gleanerDocker.sh -cfg /gleaner/wd/rundir/oih_queue.yaml  --source cioosatlantic</code></pre> <p>to run indexes on specific sources in these configuration files.  </p>"},{"location":"developement_uisng_generated_code/#references","title":"References","text":"<ul> <li>Simple Dagster example</li> </ul>"},{"location":"eco_deploy/","title":"ECO Scheduler Notes","text":"<p>Note</p> <p>these will need to become the gleanerio scheduler documentation.  for now these are rough. Images and graphics need to be loaded</p> <pre><code>flowchart TB\nPostgres_Container-- defined by --&gt; compose_project\nDagit_UI_Container-- defined by --&gt; compose_project\nDagster_Container  -- defined by --&gt; compose_project\nHeadless_Container -- defined by --&gt; compose_project\nconfigs_volume_Container -- defined by --&gt; compose_project\ncompose_project -- deployed to --&gt; docker_portainer\n\nGleaner_container -- image manual add --&gt; docker_portainer\nNabu_container -- image manual add --&gt; docker_portainer\n\nGleaner_container -- deployed by --&gt; Dagster_Container\nNabu_container -- deployed by --&gt; Dagster_Container\n\nGleaner_container--  deployed to --&gt; docker_portainer\nNabu_container--  deployed to --&gt; docker_portainer\n\nDagit_UI_Container -- Created by --&gt; Github_action\nDagster_Cotnainer -- Created by --&gt; Github_action\n\nNabuConfig.tgz -- Archive to --&gt; Nabu_container\nGleanerConfig.tfz -- Archive to --&gt; Gleaner_container\n\nNabuConfig.tgz -- Stored in s3 --&gt; s3\nGleanerConfig.tfz -- Stored in s3 --&gt; s3\n\nconfigs_volume_Container -- populates volume --&gt; dagster-project\ndagster-project -- has --&gt; gleanerConfig.yaml\ndagster-project -- has --&gt; nabuConfig.yaml</code></pre>"},{"location":"eco_deploy/#deploy","title":"Deploy","text":""},{"location":"eco_deploy/#deploy-dagster-in-portainer","title":"Deploy Dagster in Portainer","text":"<p>You will need to deploy dagster contiainers to portainer, for a docker swarm 0. get the portatinaer url, and auth token  0.  SSH to the  make hosting the docker.</p> <ol> <li>Pull scheduler repo</li> <li>cd dagster/implnets/deployment</li> <li>create a copy of envFile.env and edit env variables</li> <li>PROJECT=eco</li> <li>GLEANERIO_MINIO_ADDRESS ++</li> <li>GLEANERIO_GRAPH_URL, GLEANERIO_GRAPH_NAMESPACE</li> <li>GLEANERIO_DOCKER_URL, GLEANERIO_PORTAINER_APIKEY</li> <li>SCHED_HOSTNAME defaults to sched</li> <li>as noted as noted in (Compose, Environment and Docker API Assets), deploy the configuration to s3. </li> <li>~~create network and volumes needed <code>dagster_setup_docker.sh</code>~~</li> <li>manually add configs</li> <li>gleaner-{project}</li> <li>nabu-{project}</li> <li>workspace-{project}</li> <li>tenant-{project}</li> <li>dagster from:dagster/implnets/deployment/dagster.yaml</li> <li>add configs to S3/Minio. </li> <li>scheduler/configs/gleanerconfig.yml</li> <li>scheduler/configs/tenant.yml</li> <li>create a stack,</li> <li>gtibub repo: https://github.com/earthcube/scheduler.git</li> <li>branch: dev</li> <li>compose files: dagster/implnets/deployment/compose_project.yaml</li> <li>additional path: dagster/implnets/deployment/compose_project_eco_override.yaml</li> </ol>"},{"location":"ingest_workflow/","title":"ingest workflow","text":"<p>This is found in implnets/workflows/ingest</p> <pre><code>flowchart LR\n    subgraph dagster\n        subgraph sensors\n            s3_config_sources_sensor['sources_all_active']\n            s3_config_tenant_sensor['tenant with sources']\n            sources_sensor\n            release_file_sensor\n            tenant_names_sensor\n            tenant_namespaces_job\n        end\n        subgraph jobs\n            summon_and_release\n            sources_config_updated\n            tenant_release\n            tenant_config_updated\n        end\n        subgraph assets\n            source_names_active\n            sources_all\n            tenant_names\n            tenant_all\n        end\n\n\n    end\n    s3_config_sources_sensor--monitors --&gt; sources_config\n    s3_config_tenant_sensor--monitors  --&gt;tenant_config \n    s3_config_sources_sensor--starts--&gt;sources_config_updated\n    sources_config_updated--materializes--&gt;source_names_active\n    sources_config_updated--materializes--&gt;sources_all\n    s3_config_tenant_sensor--starts--&gt;tenant_config_updated\n    tenant_config_updated--creates--&gt;tenant_names\n    tenant_config_updated--creates--&gt;tenant_all\n    sources_sensor--monitors--&gt;sources_all\n    sources_sensor--starts--&gt;summon_and_release\n    summon_and_release--starts--&gt; gleanerio\n    gleanerio--&gt;summon\n    gleanerio--&gt;graph_path\n    tenant_names--&gt;tenant_names_sensor\n    tenant_names_sensor--starts--&gt;tenant_namespaces_job\n    tenant_namespaces_job--creates--&gt; tenant_namespace\n    tenant_namespaces_job--creates--&gt;tenant_summary_namespace\n    release_file_sensor--monitors--&gt;graph_path\n    release_file_sensor--loads--&gt;tenant_namespace\n    release_file_sensor--loads--&gt;tenant_summary_namespace\n\n    subgraph portainer\n      gleanerio \n      tenant_ui\n      subgraph services\n           subgraph triplestore\n               tenant_namespace\n               tenant_summary_namespace\n            end\n\n          subgraph minio_s3 \n            subgraph bucket_paths\n                subgraph scheduler\n                     sources_config[\"`scheduler/configs/gleanerconfig.yaml`\"]\n                     tenant_config[\"`scheduler/configs/tenant.yaml`\"]\n                     logs\n                end\n                summon\n                graph_path['graph']\n            end\n            end\n\n         end\n    end\n\n</code></pre>"},{"location":"monitoring_workflows/","title":"Monitoring workfows","text":""},{"location":"monitoring_workflows/#scheduler-interface","title":"scheduler interface","text":""},{"location":"monitoring_workflows/#check-the-run-interface","title":"check the run interface","text":"<p>http://localhost:3000/runs</p> <p>If there is a failure, click on the runid of the run, then you can look at the run log</p>"},{"location":"monitoring_workflows/#portainer-status","title":"portainer status","text":"<p>The gleaner and nabu are run as services are prefixed with sch_ pattern is</p> <p>sch_PROJECT_step</p> <p>so if it looks like something is not working, find the container starting with sch_project_step,</p> <p>then go into a terminal, </p> <p>you may need to use bin/sh</p> <pre><code>cd logs\nls -l \ntail some log name</code></pre>"},{"location":"quick/","title":"Notes","text":""},{"location":"quick/#run-deploy-dagster-locally-rough","title":"Run Deploy Dagster locally (ROUGH)","text":"<p>Dagster needs a docker instance to run Gleanerio. We usually do this in a remote container. Basically, you can run a single workflow with the UI from that workflows directory with a <code>dagster run</code></p> <p>You will need to deploy dagster contiainers to portainer, for a docker swarm 0. get the portatinaer url, and auth token  0.  SSH to the  make hosting the docker.</p> <ol> <li>Pull scheduler repo</li> <li>cd dagster/implnets/deployment</li> <li>create a copy of envFile.env and edit env variables</li> <li>PROJECT=eco</li> <li>GLEANERIO_MINIO_ADDRESS ++</li> <li>GLEANERIO_GRAPH_URL, GLEANERIO_GRAPH_NAMESPACE</li> <li>GLEANERIO_DOCKER_URL, GLEANERIO_PORTAINER_APIKEY</li> <li>SCHED_HOSTNAME defaults to sched</li> <li>as noted as noted in (Compose, Environment and Docker API Assets), deploy the configuration to s3. </li> <li>~~create network and volumes needed <code>dagster_setup_docker.sh</code>~~</li> <li>manually add configs</li> <li>gleaner-{project}</li> <li>nabu-{project}</li> <li>workspace-{project}</li> <li>tenant-{project}</li> <li>dagster from:dagster/implnets/deployment/dagster.yaml</li> <li>add configs to S3/Minio. </li> <li>scheduler/configs/gleanerconfig.yml</li> <li>scheduler/configs/tenant.yml</li> <li>then you can run a command. in runCOnfigs there are PyCharm run files</li> <li>set ENV</li> <li>GLEANERIO_GLEANER_CONFIG_PATH=/Users/valentin/development/dev_earthcube/scheduler/dagster/implnets/configs/eco/gleanerconfig.yaml</li> <li><code>cd dagster/implnets/workflows/ingest</code></li> <li><code>dagster run</code></li> </ol> <p>NEED MORE EXAMPLES</p>"},{"location":"refactor/","title":"refactoring","text":""},{"location":"refactor/#rework-code-to-generate-less","title":"rework code to generate less.","text":"<ul> <li>~~just generate the graph, and some configuration loading~~ done</li> <li>~~pass the ops the configuration~~</li> </ul>"},{"location":"refactor/#can-we-use-s3-manager-to-store-some-assets","title":"can we use s3 manager to store some assets?","text":"<ul> <li>~~reports seems like the ideal use case for these.~~ works</li> </ul>"},{"location":"refactor/#handle-multiple-workflows","title":"handle multiple workflows","text":"<ul> <li>~~need to add ability to deploy some other workflows~~ works</li> </ul>"},{"location":"refactor/#handle-multiple-organizations","title":"Handle Multiple Organizations","text":"<ul> <li>Each organization can be in a container with its own code workflow.</li> <li>If we can standardize the loading and transforming workflows as much as possible, then the graph loading workflows   should be more customizable</li> <li>to add a container, you need to edit the workflows.yaml in an organizations configuration</li> </ul>"},{"location":"refactor/#possible-workflows","title":"possible workflows","text":"<ul> <li> <p>~~timeseries after final graph~~ done</p> <ul> <li>~~generate a csv of the load reports size of (sitemap, summoned, summon failure, milled, loaded to graph, datasets)~~</li> </ul> </li> <li> <p>~~weekly summary~~ done</p> <ul> <li>~~what is the size of the graph this week.~~</li> </ul> </li> <li>post s3 check, as an approval check. <ul> <li>do these not contain JSONLD</li> <li>store as asset, or maybe have file we publish as 'approved/expected non-summoned</li> </ul> </li> <li>sitemap check<ul> <li>just run a sitemap head to see that url work, and exist, weekly.</li> <li>publish as paritioned data in s3 ;)</li> </ul> </li> <li>shacl... should we shacl releases.<ul> <li>if so, then maybe teach dagster to watch the graph/latest for changes.</li> </ul> </li> </ul>"},{"location":"assets/configs/","title":"Configs","text":""},{"location":"assets/configs/#about","title":"About","text":"<p>This directory holds some of the general configuration files that might be useful for running Gleaner and Nabu.  This section is broken down by the various communities (implementation networks).</p> <p>These are provided as examples.</p>"},{"location":"assets/configs/#tar-archive","title":"TAR archive","text":"<p>The tar archive must be compressed and must be named to align  with the archive ENV variable</p> <pre><code>GLEANERIO_GLEANER_ARCHIVE_OBJECT=scheduler/configs/GleanerCfg.tgz\nGLEANERIO_NABU_ARCHIVE_OBJECT=scheduler/configs/NabuCfg.tgz</code></pre> <pre><code> tar -zcf GleanerCfg.tgz ./gleanerconfig.yaml ./jsonldcontext.json\n ```\n\n```bash\n tar -zcf NabuCfg.tgz ./nabuconfig.yaml ./jsonldcontext.json ./assets</code></pre>"},{"location":"assets/configs/#environment-variables","title":"Environment variables","text":"<p>The command</p> <pre><code>source file.env</code></pre> <p>should set your values.  Be sure not to have spaces in  your vars.</p>"},{"location":"assets/configs/#docker-compose","title":"Docker Compose","text":"<p>Example compose file for use with Dagster is included here. It also is edited to read and express the shell variables  into the containers.</p>"},{"location":"images/appendix/","title":"Appendix","text":""},{"location":"images/appendix/#the-asset-file-flow-in-more-detail","title":"The asset file flow in more detail:","text":"<ul> <li>Creation of template files for the various operations, jobs and schedules</li> <li>Creation of the archive files that hold the configuration for the jobs run </li> <li>Environment file for the values needed by the operations</li> </ul>"}]}